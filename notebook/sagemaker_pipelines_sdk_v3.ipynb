{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines with SDK v3\n",
    "\n",
    "This notebook demonstrates how to build **MLOps pipelines** using the new **SageMaker Python SDK v3** unified APIs.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **SDK v3 Pipeline Imports** - New module structure for pipelines\n",
    "2. **Pipeline Components** - Parameters, steps, conditions, and properties\n",
    "3. **Step Types** - Processing, Training, Evaluation, Model Registration\n",
    "4. **Quality Gates** - Conditional model registration based on metrics\n",
    "5. **Model Registry** - Version control for production models\n",
    "\n",
    "## Why SageMaker Pipelines?\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| Manual ML workflows | Automated, reproducible pipelines |\n",
    "| No quality control | Metric-based gates before production |\n",
    "| Model versioning chaos | Integrated Model Registry |\n",
    "| Expensive reruns | Step caching (skip unchanged steps) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and SDK v3 Imports\n",
    "\n",
    "### SDK v3 Module Structure\n",
    "\n",
    "SDK v3 reorganized imports into logical modules:\n",
    "\n",
    "| Module | Purpose |\n",
    "|--------|--------|\n",
    "| `sagemaker.mlops.workflow` | Pipeline, steps, conditions |\n",
    "| `sagemaker.core.workflow` | Parameters, properties, functions |\n",
    "| `sagemaker.train` | ModelTrainer, training configs |\n",
    "| `sagemaker.core.processing` | ScriptProcessor, Processing I/O |\n",
    "| `sagemaker.serve` | ModelBuilder for deployment/registration |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install \"sagemaker>=3.0.0\" boto3 pandas scikit-learn xgboost joblib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SDK v3.4.1 BUG WORKAROUND\n",
    "# =============================================================================\n",
    "# There's a bug in SDK v3.4.1 where get_training_code_hash() passes\n",
    "# requirements (str) as dependencies (expects List[str]).\n",
    "# This patch must be applied BEFORE other sagemaker imports.\n",
    "# Bug tracked at: https://github.com/aws/sagemaker-python-sdk/issues/5518\n",
    "\n",
    "import sagemaker.core.workflow.utilities as _workflow_utils\n",
    "\n",
    "_original_get_training_code_hash = _workflow_utils.get_training_code_hash\n",
    "\n",
    "def _patched_get_training_code_hash(entry_point, source_dir, dependencies):\n",
    "    \"\"\"Patched version that handles string/None dependencies.\"\"\"\n",
    "    if dependencies is None:\n",
    "        dependencies = []\n",
    "    elif isinstance(dependencies, str):\n",
    "        dependencies = [dependencies] if dependencies else []\n",
    "    return _original_get_training_code_hash(entry_point, source_dir, dependencies)\n",
    "\n",
    "_workflow_utils.get_training_code_hash = _patched_get_training_code_hash\n",
    "print(\"SDK v3.4.1 bug workaround applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SDK v3 PIPELINE IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# Pipeline and steps (mlops module)\n",
    "from sagemaker.mlops.workflow.pipeline import Pipeline\n",
    "from sagemaker.mlops.workflow.steps import ProcessingStep, TrainingStep, CacheConfig\n",
    "from sagemaker.mlops.workflow.condition_step import ConditionStep\n",
    "from sagemaker.mlops.workflow.fail_step import FailStep\n",
    "from sagemaker.mlops.workflow.model_step import ModelStep\n",
    "\n",
    "# Parameters, conditions, and functions (core.workflow module)\n",
    "from sagemaker.core.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.core.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.core.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.core.workflow.properties import PropertyFile\n",
    "from sagemaker.core.workflow.functions import JsonGet\n",
    "\n",
    "# Training (train module)\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import InputData, Compute, SourceCode\n",
    "\n",
    "# Processing (core.processing module)\n",
    "from sagemaker.core.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.core.shapes.shapes import ProcessingS3Input, ProcessingS3Output\n",
    "\n",
    "# Model registration (serve module)\n",
    "from sagemaker.serve.model_builder import ModelBuilder\n",
    "\n",
    "# Session and utilities\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "from sagemaker.core.image_uris import retrieve\n",
    "\n",
    "print(\"SDK v3 pipeline imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize sessions\n",
    "sagemaker_session = Session()          # For regular operations\n",
    "pipeline_session = PipelineSession()   # For pipeline definition (deferred execution)\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Get execution role\n",
    "def get_sagemaker_role():\n",
    "    \"\"\"Get a valid SageMaker execution role.\"\"\"\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        roles = iam.list_roles()['Roles']\n",
    "        sm_roles = [r for r in roles \n",
    "                    if 'SageMaker' in r['RoleName'] \n",
    "                    and 'Execution' in r['RoleName']]\n",
    "        if sm_roles:\n",
    "            return sm_roles[0]['Arn']\n",
    "    except Exception:\n",
    "        pass\n",
    "    return get_execution_role()\n",
    "\n",
    "role = get_sagemaker_role()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Role: {role.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Pipeline Concepts\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```\n",
    "Pipeline\n",
    "├── Parameters          # Runtime inputs (instance types, thresholds, S3 paths)\n",
    "├── Steps               # Processing, Training, Evaluation, Model Registration\n",
    "│   ├── ProcessingStep  # Data preprocessing, evaluation scripts\n",
    "│   ├── TrainingStep    # Model training with ModelTrainer\n",
    "│   ├── ConditionStep   # Quality gates (if/else branching)\n",
    "│   ├── ModelStep       # Model registration to Model Registry\n",
    "│   └── FailStep        # Explicit failure with error message\n",
    "└── Properties          # Step outputs (S3 URIs, metrics) for chaining\n",
    "```\n",
    "\n",
    "### PipelineSession vs Session\n",
    "\n",
    "| Session Type | When Used | Behavior |\n",
    "|--------------|-----------|----------|\n",
    "| `Session()` | Regular operations | Executes immediately |\n",
    "| `PipelineSession()` | Pipeline definition | **Deferred** - captures args, doesn't execute |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Define Pipeline Parameters\n",
    "\n",
    "Parameters make pipelines **reusable** - change inputs without modifying code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE PARAMETERS\n",
    "# =============================================================================\n",
    "# Parameters are resolved at pipeline execution time, not definition time.\n",
    "# This allows reusing the same pipeline with different configurations.\n",
    "\n",
    "# Infrastructure parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "# Data parameters\n",
    "input_data_uri = ParameterString(\n",
    "    name=\"InputDataUri\",\n",
    "    default_value=f\"s3://{bucket}/pipeline-demo/data/\",\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "max_depth = ParameterInteger(name=\"MaxDepth\", default_value=6)\n",
    "n_estimators = ParameterInteger(name=\"NEstimators\", default_value=100)\n",
    "learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.1)\n",
    "\n",
    "# Quality gate threshold\n",
    "r2_threshold = ParameterFloat(\n",
    "    name=\"R2Threshold\",\n",
    "    default_value=0.85,  # Model must achieve R² >= 0.85 to be registered\n",
    ")\n",
    "\n",
    "# Model approval status\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\",  # Options: Approved, Rejected, PendingManualApproval\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters defined:\")\n",
    "print(f\"  - ProcessingInstanceType (default: ml.m5.xlarge)\")\n",
    "print(f\"  - TrainingInstanceType (default: ml.m5.xlarge)\")\n",
    "print(f\"  - InputDataUri (default: s3://{bucket}/pipeline-demo/data/)\")\n",
    "print(f\"  - MaxDepth (default: 6)\")\n",
    "print(f\"  - NEstimators (default: 100)\")\n",
    "print(f\"  - LearningRate (default: 0.1)\")\n",
    "print(f\"  - R2Threshold (default: 0.85)\")\n",
    "print(f\"  - ModelApprovalStatus (default: PendingManualApproval)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Configure Container Image and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SKLearn container (used for all steps - XGBoost installed via requirements.txt)\n",
    "sklearn_image = retrieve(\n",
    "    framework=\"sklearn\",\n",
    "    version=\"1.2-1\",\n",
    "    region=region,\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "print(f\"Container: {sklearn_image.split('/')[-1]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP CACHING\n",
    "# =============================================================================\n",
    "# CacheConfig enables step caching - if inputs haven't changed, skip re-execution.\n",
    "# This dramatically speeds up pipeline reruns during development.\n",
    "\n",
    "cache_config = CacheConfig(\n",
    "    enable_caching=True,\n",
    "    expire_after=\"P30D\",  # ISO 8601 duration: 30 days\n",
    ")\n",
    "print(f\"Caching enabled: 30 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Step 1: Preprocessing\n",
    "\n",
    "The preprocessing step:\n",
    "1. Loads raw data from S3\n",
    "2. Cleans and transforms features\n",
    "3. Creates train/test split\n",
    "4. Outputs to S3 for downstream steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipeline/preprocessing.py\n",
    "\"\"\"Preprocessing Script for SageMaker Pipeline\n",
    "\n",
    "Loads CSV/Parquet data, keeps numeric columns, fills NaN with 0,\n",
    "and creates 80/20 train/test split.\n",
    "\n",
    "Input: /opt/ml/processing/input/\n",
    "Output: /opt/ml/processing/train/, /opt/ml/processing/test/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data(input_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Load all CSV/Parquet files from input directory.\"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "\n",
    "    csv_files = list(input_path.glob(\"*.csv\"))\n",
    "    parquet_files = list(input_path.glob(\"*.parquet\"))\n",
    "\n",
    "    if csv_files:\n",
    "        df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "        print(f\"Loaded {len(csv_files)} CSV files\")\n",
    "    elif parquet_files:\n",
    "        df = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "        print(f\"Loaded {len(parquet_files)} Parquet files\")\n",
    "    else:\n",
    "        raise ValueError(f\"No CSV or Parquet files found in {input_dir}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame, target_column: str = \"production\") -> pd.DataFrame:\n",
    "    \"\"\"Preprocess data: keep numeric columns, fill NaN with 0.\"\"\"\n",
    "    print(f\"Raw data shape: {df.shape}\")\n",
    "\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found. Columns: {list(df.columns)}\")\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_column not in numeric_cols:\n",
    "        numeric_cols.append(target_column)\n",
    "\n",
    "    df_processed = df[numeric_cols].fillna(0)\n",
    "    print(f\"Processed data shape: {df_processed.shape}\")\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PREPROCESSING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    input_dir = \"/opt/ml/processing/input\"\n",
    "    train_dir = \"/opt/ml/processing/train\"\n",
    "    test_dir = \"/opt/ml/processing/test\"\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    df = load_data(input_dir)\n",
    "    df_processed = preprocess(df, target_column=\"production\")\n",
    "\n",
    "    train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "    print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    train_df.to_csv(os.path.join(train_dir, \"train.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(test_dir, \"test.csv\"), index=False)\n",
    "\n",
    "    print(\"PREPROCESSING COMPLETE!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROCESSING STEP\n",
    "# =============================================================================\n",
    "# Note: We use PipelineSession, not regular Session.\n",
    "# This captures the step configuration without executing it.\n",
    "\n",
    "base_job_prefix = \"pipeline-demo\"\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=sklearn_image,\n",
    "    instance_type=processing_instance_type,  # Pipeline parameter!\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_prefix}-preprocess\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,  # PipelineSession for deferred execution\n",
    ")\n",
    "\n",
    "# Define the preprocessing step\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"Preprocess\",\n",
    "    step_args=script_processor.run(\n",
    "        code=\"../pipeline/preprocessing.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                input_name=\"input\",\n",
    "                s3_input=ProcessingS3Input(\n",
    "                    s3_uri=input_data_uri,  # Pipeline parameter!\n",
    "                    s3_data_type=\"S3Prefix\",\n",
    "                    local_path=\"/opt/ml/processing/input\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                s3_output=ProcessingS3Output(\n",
    "                    s3_uri=f\"s3://{bucket}/{base_job_prefix}/processed/train\",\n",
    "                    s3_upload_mode=\"EndOfJob\",\n",
    "                    local_path=\"/opt/ml/processing/train\",\n",
    "                ),\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                s3_output=ProcessingS3Output(\n",
    "                    s3_uri=f\"s3://{bucket}/{base_job_prefix}/processed/test\",\n",
    "                    s3_upload_mode=\"EndOfJob\",\n",
    "                    local_path=\"/opt/ml/processing/test\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "print(\"Preprocessing step configured\")\n",
    "print(f\"  Input: {{InputDataUri parameter}}\")\n",
    "print(f\"  Output: s3://{bucket}/{base_job_prefix}/processed/{{train,test}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Step 2: Training\n",
    "\n",
    "The training step:\n",
    "1. Uses `ModelTrainer` (SDK v3 unified API)\n",
    "2. Reads preprocessed data from Step 1 output\n",
    "3. Trains XGBoost model\n",
    "4. Outputs model artifacts to S3\n",
    "\n",
    "### Cross-Step Data Flow\n",
    "\n",
    "```python\n",
    "# Reference output from previous step using .properties\n",
    "preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING STEP\n",
    "# =============================================================================\n",
    "\n",
    "# Configure source code (points to existing training script)\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"../training\",\n",
    "    entry_script=\"train.py\",\n",
    "    # Note: requirements not specified due to SDK v3.4.1 bug\n",
    "    # The requirements.txt in source_dir is still used at runtime\n",
    ")\n",
    "\n",
    "# Configure compute\n",
    "compute = Compute(\n",
    "    instance_type=training_instance_type,  # Pipeline parameter!\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "# Create ModelTrainer with hyperparameters from pipeline parameters\n",
    "model_trainer = ModelTrainer(\n",
    "    role=role,\n",
    "    training_image=sklearn_image,\n",
    "    source_code=source_code,\n",
    "    compute=compute,\n",
    "    base_job_name=f\"{base_job_prefix}-train\",\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": n_estimators,      # Pipeline parameter!\n",
    "        \"max-depth\": max_depth,            # Pipeline parameter!\n",
    "        \"learning-rate\": learning_rate,    # Pipeline parameter!\n",
    "        \"target-column\": \"production\",\n",
    "        \"test-size\": 0.0,  # No internal split - we already have train/test\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "# Define training step with cross-step data reference\n",
    "training_step = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    step_args=model_trainer.train(\n",
    "        input_data_config=[\n",
    "            InputData(\n",
    "                channel_name=\"train\",\n",
    "                # Reference output from preprocessing step!\n",
    "                data_source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "print(\"Training step configured\")\n",
    "print(f\"  Input: {{Preprocess step output}}\")\n",
    "print(f\"  Hyperparameters: n_estimators={{NEstimators}}, max_depth={{MaxDepth}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Step 3: Evaluation\n",
    "\n",
    "The evaluation step:\n",
    "1. Loads trained model from Step 2\n",
    "2. Runs predictions on test data from Step 1\n",
    "3. Calculates metrics (R², RMSE, MAE)\n",
    "4. Outputs `evaluation.json` for the quality gate\n",
    "\n",
    "### PropertyFile for Cross-Step Metrics\n",
    "\n",
    "```python\n",
    "# Define a PropertyFile to extract values from evaluation.json\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "# Later, use JsonGet to extract specific values\n",
    "r2_value = JsonGet(\n",
    "    step_name=\"Evaluate\",\n",
    "    property_file=evaluation_report,\n",
    "    json_path=\"regression_metrics.r2.value\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipeline/evaluation.py\n",
    "\"\"\"Model Evaluation Script for SageMaker Pipeline\n",
    "\n",
    "Extracts model from model.tar.gz, runs predictions on test data,\n",
    "and outputs evaluation.json with R2/RMSE/MAE metrics.\n",
    "\n",
    "Input:\n",
    "  - /opt/ml/processing/model/model.tar.gz\n",
    "  - /opt/ml/processing/test/*.csv\n",
    "\n",
    "Output:\n",
    "  - /opt/ml/processing/evaluation/evaluation.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "def extract_model(model_dir: str) -> str:\n",
    "    \"\"\"Extract model.tar.gz and return path to model.joblib.\"\"\"\n",
    "    tarball = os.path.join(model_dir, \"model.tar.gz\")\n",
    "\n",
    "    if not os.path.exists(tarball):\n",
    "        model_path = os.path.join(model_dir, \"model.joblib\")\n",
    "        if os.path.exists(model_path):\n",
    "            return model_path\n",
    "        raise FileNotFoundError(f\"No model.tar.gz or model.joblib in {model_dir}\")\n",
    "\n",
    "    print(f\"Extracting {tarball}\")\n",
    "    with tarfile.open(tarball, \"r:gz\") as tar:\n",
    "        tar.extractall(path=model_dir)\n",
    "\n",
    "    return os.path.join(model_dir, \"model.joblib\")\n",
    "\n",
    "\n",
    "def load_test_data(test_dir: str, target_column: str = \"production\"):\n",
    "    \"\"\"Load test data and separate features/target.\"\"\"\n",
    "    test_path = Path(test_dir)\n",
    "    csv_files = list(test_path.glob(\"*.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {test_dir}\")\n",
    "\n",
    "    df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "    print(f\"Test data shape: {df.shape}\")\n",
    "\n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=[target_column])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def save_evaluation(metrics: dict, output_dir: str):\n",
    "    \"\"\"Save evaluation.json in format expected by pipeline condition step.\n",
    "    \n",
    "    Format: {\"regression_metrics\": {\"r2\": {\"value\": 0.95}, ...}}\n",
    "    This allows JsonGet to extract values like: regression_metrics.r2.value\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    evaluation = {\n",
    "        \"regression_metrics\": {\n",
    "            \"r2\": {\"value\": metrics[\"r2\"]},\n",
    "            \"rmse\": {\"value\": metrics[\"rmse\"]},\n",
    "            \"mae\": {\"value\": metrics[\"mae\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output_path = os.path.join(output_dir, \"evaluation.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(evaluation, f, indent=2)\n",
    "\n",
    "    print(f\"Saved evaluation to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model_dir = \"/opt/ml/processing/model\"\n",
    "    test_dir = \"/opt/ml/processing/test\"\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "\n",
    "    model_path = extract_model(model_dir)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    X, y = load_test_data(test_dir, target_column=\"production\")\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    metrics = {\n",
    "        \"r2\": r2_score(y, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y, y_pred)),\n",
    "        \"mae\": mean_absolute_error(y, y_pred),\n",
    "    }\n",
    "\n",
    "    print(f\"R2: {metrics['r2']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "\n",
    "    save_evaluation(metrics, output_dir)\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION STEP\n",
    "# =============================================================================\n",
    "\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=sklearn_image,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_prefix}-evaluate\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "# Define PropertyFile to capture evaluation metrics\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"Evaluate\",\n",
    "    step_args=evaluation_processor.run(\n",
    "        code=\"../pipeline/evaluation.py\",\n",
    "        inputs=[\n",
    "            # Model artifacts from training step\n",
    "            ProcessingInput(\n",
    "                input_name=\"model\",\n",
    "                s3_input=ProcessingS3Input(\n",
    "                    s3_uri=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                    s3_data_type=\"S3Prefix\",\n",
    "                    local_path=\"/opt/ml/processing/model\",\n",
    "                ),\n",
    "            ),\n",
    "            # Test data from preprocessing step\n",
    "            ProcessingInput(\n",
    "                input_name=\"test\",\n",
    "                s3_input=ProcessingS3Input(\n",
    "                    s3_uri=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"test\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    s3_data_type=\"S3Prefix\",\n",
    "                    local_path=\"/opt/ml/processing/test\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\",\n",
    "                s3_output=ProcessingS3Output(\n",
    "                    s3_uri=f\"s3://{bucket}/{base_job_prefix}/evaluation\",\n",
    "                    s3_upload_mode=\"EndOfJob\",\n",
    "                    local_path=\"/opt/ml/processing/evaluation\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    property_files=[evaluation_report],  # Capture metrics for quality gate\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "print(\"Evaluation step configured\")\n",
    "print(f\"  Model input: {{Training step model artifacts}}\")\n",
    "print(f\"  Test input: {{Preprocess step test output}}\")\n",
    "print(f\"  Metrics output: s3://{bucket}/{base_job_prefix}/evaluation/evaluation.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Step 4: Quality Gate (Conditional Registration)\n",
    "\n",
    "The quality gate:\n",
    "1. Extracts R² from evaluation.json using `JsonGet`\n",
    "2. Compares against `R2Threshold` parameter\n",
    "3. **If R² >= threshold**: Register model to Model Registry\n",
    "4. **If R² < threshold**: Fail pipeline with error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL REGISTRATION STEP (executed only if quality gate passes)\n",
    "# =============================================================================\n",
    "\n",
    "model_package_group_name = \"PipelineDemoModels\"\n",
    "\n",
    "# Use ModelBuilder for model registration (SDK v3 pattern)\n",
    "model_builder = ModelBuilder(\n",
    "    image_uri=sklearn_image,\n",
    "    s3_model_data_url=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role_arn=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "register_step = ModelStep(\n",
    "    name=\"RegisterModel\",\n",
    "    step_args=model_builder.register(\n",
    "        content_types=[\"text/csv\", \"application/json\"],\n",
    "        response_types=[\"application/json\"],\n",
    "        inference_instances=[\"ml.t3.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,  # Pipeline parameter!\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Model registration step configured\")\n",
    "print(f\"  Model Registry group: {model_package_group_name}\")\n",
    "print(f\"  Approval status: {{ModelApprovalStatus parameter}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FAIL STEP (executed if quality gate fails)\n",
    "# =============================================================================\n",
    "\n",
    "fail_step = FailStep(\n",
    "    name=\"QualityGateFailed\",\n",
    "    error_message=\"Model R2 score is below the threshold. Model not registered.\",\n",
    ")\n",
    "\n",
    "print(\"Fail step configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONDITION STEP (quality gate)\n",
    "# =============================================================================\n",
    "\n",
    "# Extract R² value from evaluation.json\n",
    "# The JSON path must match the structure in evaluation.py:\n",
    "# {\"regression_metrics\": {\"r2\": {\"value\": 0.95}}}\n",
    "\n",
    "condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.r2.value\",  # Must match evaluation.json structure!\n",
    "    ),\n",
    "    right=r2_threshold,  # Pipeline parameter!\n",
    ")\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckR2Threshold\",\n",
    "    conditions=[condition],\n",
    "    if_steps=[register_step],   # R² >= threshold: register model\n",
    "    else_steps=[fail_step],     # R² < threshold: fail pipeline\n",
    ")\n",
    "\n",
    "print(\"Quality gate configured\")\n",
    "print(f\"  Condition: R² >= {{R2Threshold parameter}}\")\n",
    "print(f\"  If true: Register model to {model_package_group_name}\")\n",
    "print(f\"  If false: Fail with error message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Create the Pipeline\n",
    "\n",
    "Now we assemble all steps into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ASSEMBLE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "pipeline_name = \"SDKv3DemoPipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        input_data_uri,\n",
    "        model_approval_status,\n",
    "        max_depth,\n",
    "        n_estimators,\n",
    "        learning_rate,\n",
    "        r2_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        training_step,\n",
    "        evaluation_step,\n",
    "        condition_step,  # Contains register_step and fail_step\n",
    "    ],\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "print(f\"Pipeline '{pipeline_name}' assembled\")\n",
    "print(f\"  Parameters: {len(pipeline.parameters)}\")\n",
    "print(f\"  Steps: {len(pipeline.steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate pipeline definition (generates JSON)\n",
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "\n",
    "print(\"Pipeline definition:\")\n",
    "print(f\"  Parameters: {[p['Name'] for p in definition.get('Parameters', [])]}\")\n",
    "print(f\"  Steps:\")\n",
    "for step in definition.get('Steps', []):\n",
    "    print(f\"    - {step['Name']} ({step['Type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Create Sample Data and Upload to S3\n",
    "\n",
    "Before running the pipeline, we need sample data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create synthetic data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# Features that affect production\n",
    "gas_injection_rate = np.random.uniform(0.5, 3.0, n_samples)\n",
    "wellhead_pressure = np.random.uniform(100, 500, n_samples)\n",
    "temperature = np.random.uniform(50, 150, n_samples)\n",
    "water_cut = np.random.uniform(0, 0.5, n_samples)\n",
    "\n",
    "# Production with realistic relationships\n",
    "production = (\n",
    "    50  # Base production\n",
    "    + 30 * np.tanh(gas_injection_rate)  # Non-linear gas lift response\n",
    "    + 0.1 * wellhead_pressure            # Pressure effect\n",
    "    - 20 * water_cut                      # Water cut penalty\n",
    "    + np.random.normal(0, 5, n_samples)   # Noise\n",
    ")\n",
    "production = np.clip(production, 10, 150)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'gas_injection_rate': gas_injection_rate,\n",
    "    'wellhead_pressure': wellhead_pressure,\n",
    "    'temperature': temperature,\n",
    "    'water_cut': water_cut,\n",
    "    'production': production,\n",
    "})\n",
    "\n",
    "print(f\"Created synthetic data: {df.shape}\")\n",
    "print(f\"Production range: {df['production'].min():.1f} - {df['production'].max():.1f}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "from pathlib import Path\n",
    "\n",
    "local_data_dir = Path('./data')\n",
    "local_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "data_file = local_data_dir / 'production_data.csv'\n",
    "df.to_csv(data_file, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_data_key = f\"{base_job_prefix}/data/production_data.csv\"\n",
    "s3_data_uri = f\"s3://{bucket}/{s3_data_key}\"\n",
    "\n",
    "s3_client.upload_file(str(data_file), bucket, s3_data_key)\n",
    "print(f\"Uploaded to: {s3_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 11. Deploy and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE/UPDATE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "# upsert() creates the pipeline if it doesn't exist, or updates it if it does\n",
    "response = pipeline.upsert(role_arn=role)\n",
    "\n",
    "print(f\"Pipeline ARN: {response['PipelineArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# START PIPELINE EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Override default parameters for this execution\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"InputDataUri\": f\"s3://{bucket}/{base_job_prefix}/data/\",\n",
    "        \"R2Threshold\": 0.7,  # Lower threshold for demo (model should pass)\n",
    "        \"MaxDepth\": 8,\n",
    "        \"NEstimators\": 150,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Execution ARN: {execution.arn}\")\n",
    "print(f\"\\nMonitor in SageMaker Studio or run:\")\n",
    "print(f\"  execution.describe()\")\n",
    "print(f\"  execution.list_steps()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WAIT FOR COMPLETION (optional)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Waiting for pipeline execution to complete...\")\n",
    "print(\"(This typically takes 10-15 minutes for all steps)\")\n",
    "\n",
    "execution.wait()\n",
    "\n",
    "status = execution.describe()[\"PipelineExecutionStatus\"]\n",
    "print(f\"\\nFinal status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VIEW STEP RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "steps = execution.list_steps()\n",
    "\n",
    "print(\"Pipeline execution steps:\")\n",
    "for step in steps[\"PipelineExecutionSteps\"]:\n",
    "    status = step[\"StepStatus\"]\n",
    "    emoji = \"✅\" if status == \"Succeeded\" else \"❌\" if status == \"Failed\" else \"⏳\"\n",
    "    print(f\"  {emoji} {step['StepName']}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 12. View Registered Model in Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VIEW MODEL REGISTRY\n",
    "# =============================================================================\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# List model packages in the group\n",
    "model_packages = sm_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=5\n",
    ")\n",
    "\n",
    "print(f\"Model packages in '{model_package_group_name}':\")\n",
    "for pkg in model_packages.get('ModelPackageSummaryList', []):\n",
    "    print(f\"  Version: {pkg['ModelPackageVersion']}\")\n",
    "    print(f\"    Status: {pkg['ModelApprovalStatus']}\")\n",
    "    print(f\"    Created: {pkg['CreationTime']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPROVE MODEL FOR PRODUCTION (optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment to approve the latest model\n",
    "# if model_packages.get('ModelPackageSummaryList'):\n",
    "#     latest_arn = model_packages['ModelPackageSummaryList'][0]['ModelPackageArn']\n",
    "#     sm_client.update_model_package(\n",
    "#         ModelPackageArn=latest_arn,\n",
    "#         ModelApprovalStatus=\"Approved\"\n",
    "#     )\n",
    "#     print(f\"Approved: {latest_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## 13. Test the Quality Gate\n",
    "\n",
    "Run with a high threshold to see the quality gate fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST QUALITY GATE FAILURE\n",
    "# =============================================================================\n",
    "\n",
    "# Run with threshold of 0.99 - model won't achieve this, so pipeline should fail\n",
    "fail_execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"InputDataUri\": f\"s3://{bucket}/{base_job_prefix}/data/\",\n",
    "        \"R2Threshold\": 0.99,  # Unrealistically high - should fail\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Started execution with R2Threshold=0.99\")\n",
    "print(f\"Execution ARN: {fail_execution.arn}\")\n",
    "print(f\"\\nThis execution should fail at the quality gate.\")\n",
    "print(f\"Check status with: fail_execution.describe()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## 14. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP (uncomment to run)\n",
    "# =============================================================================\n",
    "\n",
    "# Delete pipeline\n",
    "# pipeline.delete()\n",
    "# print(f\"Deleted pipeline: {pipeline_name}\")\n",
    "\n",
    "# Delete S3 data\n",
    "# !aws s3 rm s3://{bucket}/{base_job_prefix} --recursive\n",
    "# print(f\"Deleted S3 data: s3://{bucket}/{base_job_prefix}\")\n",
    "\n",
    "print(\"Cleanup commands (uncomment to run):\")\n",
    "print(f\"  pipeline.delete()\")\n",
    "print(f\"  aws s3 rm s3://{bucket}/{base_job_prefix} --recursive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### SDK v3 Pipeline Imports\n",
    "\n",
    "```python\n",
    "# Pipeline and steps\n",
    "from sagemaker.mlops.workflow.pipeline import Pipeline\n",
    "from sagemaker.mlops.workflow.steps import ProcessingStep, TrainingStep, CacheConfig\n",
    "from sagemaker.mlops.workflow.condition_step import ConditionStep\n",
    "from sagemaker.mlops.workflow.fail_step import FailStep\n",
    "from sagemaker.mlops.workflow.model_step import ModelStep\n",
    "\n",
    "# Parameters and conditions\n",
    "from sagemaker.core.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.core.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.core.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.core.workflow.properties import PropertyFile\n",
    "from sagemaker.core.workflow.functions import JsonGet\n",
    "\n",
    "# Training\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import InputData, Compute, SourceCode\n",
    "\n",
    "# Processing\n",
    "from sagemaker.core.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.core.shapes.shapes import ProcessingS3Input, ProcessingS3Output\n",
    "\n",
    "# Model registration\n",
    "from sagemaker.serve.model_builder import ModelBuilder\n",
    "```\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "| Pattern | Description |\n",
    "|---------|-------------|\n",
    "| `PipelineSession()` | Deferred execution for pipeline definition |\n",
    "| `step.properties.X` | Reference outputs from previous steps |\n",
    "| `PropertyFile` + `JsonGet` | Extract values from JSON for conditions |\n",
    "| `CacheConfig` | Skip unchanged steps (30-day cache) |\n",
    "| `ModelBuilder.register()` | Register model to Model Registry |\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```\n",
    "Preprocess → Train → Evaluate → [R² >= threshold?]\n",
    "                                    ↓ Yes        ↓ No\n",
    "                              RegisterModel   FailStep\n",
    "```\n",
    "\n",
    "### Known Issues (SDK v3.4.1)\n",
    "\n",
    "- **Bug**: `get_training_code_hash()` expects `List[str]` but receives `str`\n",
    "- **Workaround**: Monkey-patch at top of script (see cell 3)\n",
    "- **Tracked**: https://github.com/aws/sagemaker-python-sdk/issues/5518"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
