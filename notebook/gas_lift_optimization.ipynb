{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Training with Amazon SageMaker Python SDK v3\n",
    "\n",
    "This notebook demonstrates **model training and deployment** using the new **SageMaker Python SDK v3** unified APIs.\n",
    "\n",
    "## SDK v3 vs SDK v2\n",
    "\n",
    "| Task | Legacy SDK v2 | SDK v3 |\n",
    "|------|--------------|--------|\n",
    "| Training | `SKLearn()`, `XGBoost()`, `PyTorch()` | `ModelTrainer` |\n",
    "| Input Data | `TrainingInput` | `InputData` |\n",
    "| Compute | Inline parameters | `Compute` config class |\n",
    "| Deployment | `model.deploy()` | `Model.create` + `Endpoint.create` |\n",
    "\n",
    "## Use Case: Gas Lift Optimization\n",
    "\n",
    "**Gas lift** is an artificial lift method where compressed gas is injected into oil wells to reduce fluid density and increase production.\n",
    "\n",
    "**ML task**: Predict oil production given well sensor readings and gas injection rates (regression).\n",
    "\n",
    "**Data**: [Petrobras 3W dataset](https://github.com/petrobras/3W) - real sensor data from offshore wells.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sagemaker boto3 pandas numpy xgboost scipy pyarrow joblib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import boto3\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport os\nimport tarfile\nimport shutil\nfrom datetime import datetime\n\n# Amazon SageMaker AI SDK v3 imports\nfrom sagemaker.train.model_trainer import ModelTrainer\nfrom sagemaker.train.configs import InputData, Compute, SourceCode, OutputDataConfig\nfrom sagemaker.core.helper.session_helper import Session, get_execution_role\nfrom sagemaker.core.image_uris import retrieve\n\n# Initialize Amazon SageMaker AI session\nsagemaker_session = Session()\nregion = sagemaker_session.boto_region_name\nbucket = sagemaker_session.default_bucket()\nprefix = 'gas-lift-optimization'\n\n# Get execution role - prefer explicit SageMaker role, fall back to get_execution_role()\ndef get_sagemaker_role():\n    \"\"\"Get a valid SageMaker execution role.\"\"\"\n    # First, search for an explicit SageMaker execution role\n    iam = boto3.client('iam')\n    try:\n        roles = iam.list_roles()['Roles']\n        sm_roles = [r for r in roles \n                    if 'SageMaker' in r['RoleName'] \n                    and 'Execution' in r['RoleName']\n                    and 'service-role' in r['Arn']]\n        if sm_roles:\n            return sm_roles[0]['Arn']\n    except Exception:\n        pass\n    \n    # Fall back to get_execution_role() (works in SageMaker notebooks)\n    try:\n        return get_execution_role()\n    except ValueError:\n        pass\n    \n    raise ValueError(\n        \"No SageMaker execution role found. \"\n        \"Create one in IAM with AmazonSageMakerFullAccess policy.\"\n    )\n\nrole = get_sagemaker_role()\n\nprint(f\"SageMaker Python SDK v3\")\nprint(f\"Region: {region}\")\nprint(f\"Role: {role.split('/')[-1]}\")\nprint(f\"Amazon S3 Bucket: {bucket}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Scripts\n",
    "\n",
    "The training scripts are in the `../training/` directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to training code directory\n",
    "source_dir = Path('../training').resolve()\n",
    "\n",
    "print(f\"Training code directory: {source_dir}\")\n",
    "for f in source_dir.iterdir():\n",
    "    print(f\"  {f.name} ({f.stat().st_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data\n",
    "\n",
    "Using the open-source **Petrobras 3W dataset** - real sensor data from gas-lifted offshore wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone 3W dataset if not present\n",
    "data_path = Path('./3W')\n",
    "if not data_path.exists():\n",
    "    print(\"Cloning 3W dataset (Petrobras open-source well data)...\")\n",
    "    !git clone https://github.com/petrobras/3W.git\n",
    "else:\n",
    "    print(f\"Dataset exists at {data_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load well files (class 3 = Severe Slugging, has gas lift rate variation)\n",
    "dataset_path = data_path / 'dataset' / '3'\n",
    "real_files = sorted([f for f in dataset_path.glob('WELL-*.parquet')])\n",
    "print(f\"Found {len(real_files)} well files\")\n",
    "\n",
    "dfs = []\n",
    "for i, f in enumerate(real_files):\n",
    "    df = pd.read_parquet(f, engine='pyarrow')\n",
    "    df['file_id'] = i\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = df.reset_index()\n",
    "    dfs.append(df)\n",
    "    if i < 3:\n",
    "        print(f\"  {f.name}: {len(df):,} rows\")\n",
    "\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nTotal: {len(all_data):,} rows from {len(dfs)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features with good data availability\n",
    "exclude_cols = ['timestamp', 'class', 'state', 'file_id', 'well_name']\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "feature_cols = [col for col in numeric_cols \n",
    "                if col not in exclude_cols \n",
    "                and all_data[col].notna().sum() / len(all_data) > 0.5\n",
    "                and all_data[col].std() > 0]\n",
    "\n",
    "print(f\"Selected {len(feature_cols)} features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clean data with synthetic production target\n",
    "clean_data = all_data[feature_cols].copy()\n",
    "for col in feature_cols:\n",
    "    clean_data[col] = clean_data[col].fillna(clean_data[col].median())\n",
    "\n",
    "# Create synthetic production target (in production, use actual well test data)\n",
    "np.random.seed(42)\n",
    "well_ids = all_data['file_id'].values[:len(clean_data)]\n",
    "n_wells = int(well_ids.max()) + 1\n",
    "well_efficiency = np.random.uniform(0.5, 1.5, n_wells)\n",
    "\n",
    "# Gas lift effect (non-linear response)\n",
    "if 'QGL' in feature_cols:\n",
    "    qgl = clean_data['QGL'].values\n",
    "    qgl_norm = (qgl - qgl.mean()) / (qgl.std() + 1e-6)\n",
    "    gl_effect = 30 * np.tanh(qgl_norm) * well_efficiency[well_ids.astype(int)]\n",
    "else:\n",
    "    gl_effect = 0\n",
    "\n",
    "# Pressure effect\n",
    "p_cols = [c for c in feature_cols if c.startswith('P-')]\n",
    "p_effect = 20 * ((clean_data[p_cols[0]] - clean_data[p_cols[0]].mean()) / clean_data[p_cols[0]].std()) if p_cols else 0\n",
    "\n",
    "production = 100 + gl_effect + p_effect + np.random.normal(0, 5, len(clean_data))\n",
    "clean_data['production'] = np.clip(production, 10, 200)\n",
    "\n",
    "print(f\"Data shape: {clean_data.shape}\")\n",
    "print(f\"Production range: {clean_data['production'].min():.1f} - {clean_data['production'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Data to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and upload training data\n",
    "local_data_dir = Path('./data')\n",
    "local_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_file = local_data_dir / 'train.csv'\n",
    "clean_data.to_csv(train_file, index=False)\n",
    "print(f\"Saved: {train_file} ({train_file.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "train_s3_key = f\"{prefix}/data/train/train.csv\"\n",
    "train_s3_uri = f\"s3://{bucket}/{train_s3_key}\"\n",
    "\n",
    "s3_client.upload_file(str(train_file), bucket, train_s3_key)\n",
    "print(f\"Uploaded to: {train_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Amazon SageMaker AI Training Job\n",
    "\n",
    "Using **SageMaker Python SDK v3** with:\n",
    "- `ModelTrainer` - unified training API\n",
    "- SKLearn 1.4-2 container with custom requirements.txt for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SKLearn container\n",
    "sklearn_image = retrieve(\n",
    "    framework='sklearn',\n",
    "    region=region,\n",
    "    version='1.4-2',\n",
    "    py_version='py3',\n",
    "    image_scope='training'\n",
    ")\n",
    "print(f\"Training container: {sklearn_image.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training job\n",
    "hyperparameters = {\n",
    "    'n-estimators': '100',\n",
    "    'max-depth': '6',\n",
    "    'learning-rate': '0.1',\n",
    "    'target-column': 'production'\n",
    "}\n",
    "\n",
    "source_code_config = SourceCode(\n",
    "    source_dir=str(source_dir),\n",
    "    entry_script='train.py',\n",
    "    requirements='requirements.txt'\n",
    ")\n",
    "\n",
    "compute_config = Compute(\n",
    "    instance_type='ml.m5.large',\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=30\n",
    ")\n",
    "\n",
    "output_s3_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=sklearn_image,\n",
    "    source_code=source_code_config,\n",
    "    compute=compute_config,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    base_job_name='gaslift-xgboost',\n",
    "    output_data_config=OutputDataConfig(s3_output_path=output_s3_path),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(f\"ModelTrainer configured\")\n",
    "print(f\"  Instance: {compute_config.instance_type}\")\n",
    "print(f\"  Output: {output_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training input and start training\n",
    "training_input = InputData(\n",
    "    channel_name='train',\n",
    "    data_source=train_s3_uri,\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING AMAZON SAGEMAKER AI TRAINING JOB\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "print(\"\\nThis will provision an ml.m5.large instance and run training.\")\n",
    "print(\"Please wait (typically 3-5 minutes)...\\n\")\n",
    "\n",
    "model_trainer.train(\n",
    "    input_data_config=[training_input],\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job results\n",
    "sm_client = boto3.client('sagemaker')\n",
    "latest_job = sm_client.list_training_jobs(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=1,\n",
    "    NameContains='gaslift'\n",
    ")['TrainingJobSummaries'][0]\n",
    "\n",
    "training_job_name = latest_job['TrainingJobName']\n",
    "job_desc = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_artifacts = job_desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Job: {training_job_name}\")\n",
    "print(f\"Status: {latest_job['TrainingJobStatus']}\")\n",
    "print(f\"Model: {model_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download and Verify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifacts\n",
    "model_dir = Path('./models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_s3_path = model_artifacts.replace(f's3://{bucket}/', '')\n",
    "local_model_tar = model_dir / 'model.tar.gz'\n",
    "\n",
    "print(f\"Downloading model...\")\n",
    "s3_client.download_file(bucket, model_s3_path, str(local_model_tar))\n",
    "\n",
    "with tarfile.open(local_model_tar, 'r:gz') as tar:\n",
    "    tar.extractall(model_dir)\n",
    "\n",
    "print(f\"Model extracted to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify model\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "model = joblib.load(model_dir / 'model.joblib')\n",
    "with open(model_dir / 'feature_names.json', 'r') as f:\n",
    "    trained_features = json.load(f)['feature_names']\n",
    "\n",
    "print(f\"Model: {type(model).__name__}\")\n",
    "print(f\"Features: {len(trained_features)}\")\n",
    "\n",
    "# Validate\n",
    "X_val = clean_data[trained_features].head(10000)\n",
    "y_val = clean_data['production'].head(10000)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  R2 Score: {r2_score(y_val, y_pred):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_val, y_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy Model to Real-time Endpoint\n",
    "\n",
    "Deploy the trained model to a SageMaker real-time endpoint for inference.\n",
    "\n",
    "**Important**: The SKLearn container requires proper inference code bundled in the model artifact:\n",
    "- Model files at root level (`model.joblib`, `feature_names.json`)\n",
    "- Inference code in `code/` subdirectory (`inference.py`, `requirements.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to inference code directory\n",
    "inference_dir = Path('../inference').resolve()\n",
    "\n",
    "print(f\"Inference code directory: {inference_dir}\")\n",
    "for f in inference_dir.iterdir():\n",
    "    print(f\"  {f.name} ({f.stat().st_size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repack model with inference code for deployment\n",
    "repack_dir = model_dir / 'repacked'\n",
    "code_dir = repack_dir / 'code'\n",
    "code_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy model files to root\n",
    "shutil.copy(model_dir / 'model.joblib', repack_dir / 'model.joblib')\n",
    "shutil.copy(model_dir / 'feature_names.json', repack_dir / 'feature_names.json')\n",
    "\n",
    "# Copy inference code to code/ subdirectory\n",
    "shutil.copy(inference_dir / 'inference.py', code_dir / 'inference.py')\n",
    "shutil.copy(inference_dir / 'requirements.txt', code_dir / 'requirements.txt')\n",
    "\n",
    "# Create new model tarball\n",
    "repacked_tar = model_dir / 'model_deploy.tar.gz'\n",
    "with tarfile.open(repacked_tar, 'w:gz') as tar:\n",
    "    for item in repack_dir.iterdir():\n",
    "        tar.add(item, arcname=item.name)\n",
    "\n",
    "print(f\"Created deployment package: {repacked_tar}\")\n",
    "print(f\"Contents:\")\n",
    "with tarfile.open(repacked_tar, 'r:gz') as tar:\n",
    "    for member in tar.getmembers():\n",
    "        print(f\"  ./{member.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload repacked model to S3\n",
    "model_s3_key = f\"{prefix}/models/model_deploy.tar.gz\"\n",
    "deploy_model_s3_uri = f\"s3://{bucket}/{model_s3_key}\"\n",
    "\n",
    "s3_client.upload_file(str(repacked_tar), bucket, model_s3_key)\n",
    "print(f\"Uploaded to: {deploy_model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deploy to real-time endpoint using SDK v3\nfrom sagemaker.core.resources import Model as SageMakerModel\nfrom sagemaker.core.shapes import ContainerDefinition\n\n# Get inference container (note: 1.2-1 is latest for inference, 1.4-2 is training-only)\nsklearn_inference_image = retrieve(\n    framework='sklearn',\n    region=region,\n    version='1.2-1',  # Latest available for inference\n    py_version='py3',\n    image_scope='inference'\n)\n\n# Create unique names\ntimestamp = datetime.now().strftime('%Y%m%d%H%M%S')\nmodel_name = f'gaslift-mdl-{timestamp}'\nendpoint_name = f'gaslift-ep-{timestamp}'\n\n# Create container definition with required environment variables\ncontainer = ContainerDefinition(\n    image=sklearn_inference_image,\n    model_data_url=deploy_model_s3_uri,\n    environment={\n        'SAGEMAKER_PROGRAM': 'inference.py',\n        'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n    }\n)\n\n# Create model\nsm_model = SageMakerModel.create(\n    model_name=model_name,\n    primary_container=container,\n    execution_role_arn=role,\n)\nprint(f\"Created model: {model_name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint configuration and deploy\n",
    "from sagemaker.core.resources import EndpointConfig, Endpoint\n",
    "from sagemaker.core.shapes import ProductionVariant\n",
    "import time\n",
    "\n",
    "# Create endpoint config\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=endpoint_name,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name='AllTraffic',\n",
    "            model_name=model_name,\n",
    "            initial_instance_count=1,\n",
    "            instance_type='ml.t2.medium',\n",
    "            initial_variant_weight=1.0,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(f\"Created endpoint config: {endpoint_name}\")\n",
    "\n",
    "# Create endpoint\n",
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_config_name=endpoint_name,\n",
    ")\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "print(\"Waiting for endpoint to be InService (typically 3-5 minutes)...\")\n",
    "\n",
    "# Wait for endpoint to be ready\n",
    "while True:\n",
    "    status = sm_client.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n",
    "    print(f\"  {datetime.now().strftime('%H:%M:%S')} - Status: {status}\")\n",
    "    if status == 'InService':\n",
    "        print(\"\\nEndpoint is ready!\")\n",
    "        break\n",
    "    elif status == 'Failed':\n",
    "        raise Exception(f\"Endpoint deployment failed\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deployed endpoint\n",
    "runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "\n",
    "print(f\"Testing endpoint: {endpoint_name}\")\n",
    "print(f\"Features ({len(trained_features)}): {trained_features}\")\n",
    "\n",
    "# Create test samples using actual data\n",
    "test_samples = clean_data[trained_features].sample(3, random_state=42).values.tolist()\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=json.dumps(test_samples)\n",
    ")\n",
    "\n",
    "predictions = json.loads(response['Body'].read().decode())\n",
    "print(f\"\\nPredictions:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"  Sample {i+1}: {pred:.4f} bbl/d\")\n",
    "\n",
    "print(\"\\nEndpoint working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up endpoint (uncomment to run)\n",
    "# print(\"Cleaning up endpoint resources...\")\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "# sm_client.delete_model(ModelName=model_name)\n",
    "# print(\"Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gas Lift Optimization\n",
    "\n",
    "Now we use the trained model concept to optimize gas allocation across multiple wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "class Well:\n",
    "    \"\"\"Represents a gas lift well with its response curve.\"\"\"\n",
    "    def __init__(self, name, base_production, efficiency, max_response):\n",
    "        self.name = name\n",
    "        self.base_production = base_production\n",
    "        self.efficiency = efficiency\n",
    "        self.max_response = max_response\n",
    "    \n",
    "    def production(self, gas_rate):\n",
    "        \"\"\"Non-linear production response to gas injection.\"\"\"\n",
    "        return self.base_production + self.max_response * np.tanh(gas_rate / 1.5 * self.efficiency)\n",
    "\n",
    "\n",
    "class GasLiftOptimizer:\n",
    "    \"\"\"Optimizes gas allocation across multiple wells.\"\"\"\n",
    "    def __init__(self, wells):\n",
    "        self.wells = wells\n",
    "    \n",
    "    def total_production(self, gas_alloc):\n",
    "        return sum(w.production(g) for w, g in zip(self.wells, gas_alloc))\n",
    "    \n",
    "    def optimize(self, total_gas, min_gas=0.3, max_gas=2.0):\n",
    "        n = len(self.wells)\n",
    "        bounds = [(min_gas, max_gas)] * n\n",
    "        constraints = {'type': 'ineq', 'fun': lambda x: total_gas - sum(x)}\n",
    "        x0 = np.array([total_gas / n] * n)\n",
    "        \n",
    "        result = minimize(lambda x: -self.total_production(x), x0, \n",
    "                         method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        return {\n",
    "            'baseline_alloc': x0,\n",
    "            'optimal_alloc': result.x,\n",
    "            'baseline_prod': self.total_production(x0),\n",
    "            'optimal_prod': self.total_production(result.x),\n",
    "            'improvement_pct': (self.total_production(result.x) - self.total_production(x0)) / \n",
    "                               self.total_production(x0) * 100\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wells with different gas lift efficiencies\n",
    "wells = [\n",
    "    Well(\"WELL-A\", 80, 0.3, 80),   # Poor responder\n",
    "    Well(\"WELL-B\", 70, 0.5, 100),  # Below average\n",
    "    Well(\"WELL-C\", 90, 1.0, 120),  # Average\n",
    "    Well(\"WELL-D\", 100, 1.5, 150), # Good responder\n",
    "    Well(\"WELL-E\", 85, 2.0, 180),  # Excellent responder\n",
    "]\n",
    "\n",
    "# Optimize gas allocation\n",
    "optimizer = GasLiftOptimizer(wells)\n",
    "result = optimizer.optimize(total_gas=6.0, min_gas=0.3, max_gas=2.0)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"GAS LIFT OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Well':<10} {'Efficiency':<12} {'Gas (Base)':<12} {'Gas (Opt)':<12} {'Change':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, well in enumerate(wells):\n",
    "    gb, go = result['baseline_alloc'][i], result['optimal_alloc'][i]\n",
    "    print(f\"{well.name:<10} {well.efficiency:<12.1f} {gb:<12.2f} {go:<12.2f} {go-gb:+10.2f}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nBaseline Production: {result['baseline_prod']:.1f} bbl/d\")\n",
    "print(f\"Optimized Production: {result['optimal_prod']:.1f} bbl/d\")\n",
    "print(f\"\\n>>> IMPROVEMENT: {result['improvement_pct']:+.1f}% <<<\")\n",
    "\n",
    "# Business value\n",
    "oil_price = 70  # $/bbl\n",
    "daily_gain = result['optimal_prod'] - result['baseline_prod']\n",
    "annual_value = daily_gain * 365 * oil_price\n",
    "print(f\"\\nAnnual Value: ${annual_value:,.0f}/year at ${oil_price}/bbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ARTIFACTS CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAmazon S3:\")\n",
    "print(f\"  Training data: {train_s3_uri}\")\n",
    "print(f\"  Model artifacts: {model_artifacts}\")\n",
    "print(f\"  Deploy model: {deploy_model_s3_uri}\")\n",
    "print(f\"\\nLocal:\")\n",
    "print(f\"  Training code: {source_dir}\")\n",
    "print(f\"  Model: {model_dir.absolute()}\")\n",
    "print(f\"\\nEndpoint:\")\n",
    "print(f\"  Name: {endpoint_name}\")\n",
    "print(f\"\\nTo delete all S3 data:\")\n",
    "print(f\"  aws s3 rm s3://{bucket}/{prefix} --recursive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **SageMaker Python SDK v3** for training and deployment:\n",
    "\n",
    "### SDK v3 Classes Used\n",
    "\n",
    "| Class | Purpose | Import |\n",
    "|-------|---------|--------|\n",
    "| `ModelTrainer` | Training jobs | `sagemaker.train.model_trainer` |\n",
    "| `SourceCode` | Script + dependencies | `sagemaker.train.configs` |\n",
    "| `Compute` | Instance configuration | `sagemaker.train.configs` |\n",
    "| `InputData` | Training data channels | `sagemaker.train.configs` |\n",
    "| `OutputDataConfig` | Model output location | `sagemaker.train.configs` |\n",
    "| `Model` | Model resource | `sagemaker.core.resources` |\n",
    "| `EndpointConfig` | Endpoint configuration | `sagemaker.core.resources` |\n",
    "| `Endpoint` | Real-time endpoint | `sagemaker.core.resources` |\n",
    "| `retrieve` | Container image URIs | `sagemaker.core.image_uris` |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Unified Training API** - `ModelTrainer` replaces framework-specific estimators\n",
    "2. **Custom Dependencies** - Use `requirements.txt` with SKLearn container\n",
    "3. **Model Packaging for Deployment** - SKLearn containers require:\n",
    "   - Model files at root (`model.joblib`)\n",
    "   - Inference code in `code/` subdirectory (`inference.py`, `requirements.txt`)\n",
    "   - Environment variables: `SAGEMAKER_PROGRAM`, `SAGEMAKER_SUBMIT_DIRECTORY`\n",
    "\n",
    "### Repository Structure\n",
    "\n",
    "```\n",
    "├── training/\n",
    "│   ├── train.py           # Training script\n",
    "│   └── requirements.txt   # Training dependencies\n",
    "└── inference/\n",
    "    ├── inference.py       # Inference handlers\n",
    "    └── requirements.txt   # Inference dependencies\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Batch Inference**: Use `Transformer` for large-scale predictions\n",
    "- **MLOps**: Use [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html) for automation\n",
    "- **Model Registry**: Track model versions with SageMaker Model Registry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}